[START]
   ↓
[Load Data]
   - Read CSV file (student_exam_data.csv) using pandas
   - Input features: X = ["Study Hours", "Previous Exam Score"] (shape: n_samples, 2)
   - Target: y = ["Pass/Fail"] (shape: n_samples, 1)
   - Convert X and y to torch.float32 tensors
   ↓
[Define Model]
   - Initialize model with nn.Sequential:
     - Layer 1: nn.Linear(2, 10) (hidden layer, weights: (10, 2), bias: (10,))
     - Layer 2: nn.ReLU() (activation function)
     - Layer 3: nn.Linear(10, 1) (output layer, weights: (1, 10), bias: (1,))
   - Print model structure
   - Define loss function: BCEWithLogitsLoss
   - Initialize optimizer: SGD with lr=0.005, using model.parameters()
   ↓
[Training Loop (i = 0 to 49,999)]
   ↓
[Zero Gradients]
   - optimizer.zero_grad()
   ↓
[Forward Pass]
   - Compute model output: logits = model(X)
     - Step 1: Linear(2, 10): X @ W1 + b1
     - Step 2: ReLU activation: max(0, hidden_outputs)
     - Step 3: Linear(10, 1): hidden_outputs @ W2 + b2
   - Output: logits
   ↓
[Compute Loss]
   - loss = loss_fn(logits, y) (BCEWithLogitsLoss)
   ↓
[Backpropagation]
   - loss.backward() (Compute gradients for weights and biases)
   ↓
[Update Parameters]
   - optimizer.step() (Update weights and biases)
   ↓
[Check Iteration]
   - If i % 10,000 == 0:
     → [Print Loss]
         - Print loss value (loss.item())
         ↓
     ← [Return to Loop]
   - Else:
     ↓
[End of Loop?]
   - If i < 50,000:
     → [Return to Zero Gradients]
   - Else:
     ↓
[Set Evaluation Mode]
   - model.eval()
   ↓
[Evaluation (with torch.no_grad())]
   - Compute logits: model(X)
   - Compute predictions: preds = torch.sigmoid(model(X)) > 0.5
   - Compute accuracy: mean(preds.float() == y)
   ↓
[Print Accuracy]
   - Print accuracy value
   ↓
[END]